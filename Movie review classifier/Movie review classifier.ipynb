{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00YI_bYSr3Lg"
      },
      "source": [
        "# Text classification from scratch\n",
        "\n",
        "**Authors:** Mark Omernick, Francois Chollet<br>\n",
        "**Date created:** 2019/11/06<br>\n",
        "**Last modified:** 2020/05/17<br>\n",
        "**Description:** Text sentiment classification starting from raw text files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKG-5EX2r3Lk"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example shows how to do text classification starting from raw text (as\n",
        "a set of text files on disk). We demonstrate the workflow on the IMDB sentiment\n",
        "classification dataset (unprocessed version). We use the `TextVectorization` layer for\n",
        " word splitting & indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOGK-mkr3Ll"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3WS-dTrTr3Ll"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, AveragePooling1D, Bidirectional, LSTM, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoJVaGiwr3Ln"
      },
      "source": [
        "## Load the data: IMDB movie review sentiment classification\n",
        "\n",
        "Let's download the data and inspect its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c-N9_VLgr3Ln",
        "outputId": "0d955305-e5bb-4025-f2d6-46a282e656fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  10.0M      0  0:00:07  0:00:07 --:--:-- 17.7M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPBLF6rqr3Lo"
      },
      "source": [
        "The `aclImdb` folder contains a `train` and `test` subfolder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sCClYoBVr3Lo",
        "outputId": "77237fe2-53c1-4e11-d455-3aad9e248e0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eRcoHyCYr3Lo",
        "outputId": "4f1731dc-70ee-4ecc-e3d9-55a696d31bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tuiJd2UQr3Lp",
        "outputId": "f16acf7d-e8cc-4872-84b6-bda4d8a831bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dOmRvn8r3Lp"
      },
      "source": [
        "The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each of\n",
        " which represents one review (either positive or negative):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0JVxBQN4r3Lp",
        "outputId": "c497195e-7dc9-46f6-ab4f-40c789f15932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqUY__02r3Lq"
      },
      "source": [
        "We are only interested in the `pos` and `neg` subfolders, so let's delete the rest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jkwqvShCr3Lq"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for dir in range (10):\n",
        "    os.mkdir(f'aclImdb/train/{dir}')"
      ],
      "metadata": {
        "id": "5A8X8CwGRhUe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dir in range (10):\n",
        "    os.mkdir(f'aclImdb/test/{dir}')\n",
        "!ls aclImdb/test "
      ],
      "metadata": {
        "id": "Efxio7onR7CQ",
        "outputId": "3a040382-6f57-4289-d545-4a824dd138fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  2  4  6  8  labeledBow.feat\tpos\t      urls_pos.txt\n",
            "1  3  5  7  9  neg\t\turls_neg.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ9hCWZjr3Lq"
      },
      "source": [
        "You can use the utility `tf.keras.utils.text_dataset_from_directory` to\n",
        "generate a labeled `tf.data.Dataset` object from a set of text files on disk filed\n",
        " into class-specific folders.\n",
        "\n",
        "Let's use it to generate the training, validation, and test datasets. The validation\n",
        "and training datasets are generated from two subsets of the `train` directory, with 20%\n",
        "of samples going to the validation dataset and 80% going to the training dataset.\n",
        "\n",
        "Having a validation dataset in addition to the test dataset is useful for tuning\n",
        "hyperparameters, such as the model architecture, for which the test dataset should not\n",
        "be used.\n",
        "\n",
        "Before putting the model out into the real world however, it should be retrained using all\n",
        "available training data (without creating a validation dataset), so its performance is maximized.\n",
        "\n",
        "When using the `validation_split` & `subset` arguments, make sure to either specify a\n",
        "random seed, or to pass `shuffle=False`, so that the validation & training splits you\n",
        "get have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = 'aclImdb/train'\n",
        " \n",
        "for root, dirs, files in os.walk(directory):\n",
        "    if root == 'aclImdb/train/neg' or root== 'aclImdb/train/pos':\n",
        "        for file_ in files:\n",
        "            if file_[-5]!='0':\n",
        "                dir_ = int(file_[-5])-1\n",
        "            else: \n",
        "                dir_ = int(file_[-6:-4])-1\n",
        "\n",
        "            os.rename(''.join([root,'/',file_]), f'aclImdb/train/{dir_}/{file_}')"
      ],
      "metadata": {
        "id": "dCY9hUqti_59"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/neg\n",
        "!rm -r aclImdb/train/pos"
      ],
      "metadata": {
        "id": "oPluEvVQUozn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb/train "
      ],
      "metadata": {
        "id": "30C5Ca3kTeap",
        "outputId": "731cab36-bbd2-4af9-eb4d-f8f3f27d3e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  2  4  6  8  labeledBow.feat\turls_neg.txt  urls_unsup.txt\n",
            "1  3  5  7  9  unsupBow.feat\turls_pos.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")"
      ],
      "metadata": {
        "id": "Xdutx-mMUb6I",
        "outputId": "297456de-e192-49f5-89df-225af89931bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 10 classes.\n",
            "Using 20000 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = 'aclImdb/test'\n",
        " \n",
        "for root, dirs, files in os.walk(directory):\n",
        "    if root == 'aclImdb/test/neg' or root== 'aclImdb/test/pos':\n",
        "        for file_ in files:\n",
        "            if file_[-5]!='0':\n",
        "                dir_ = int(file_[-5])-1\n",
        "            else: \n",
        "                dir_ = int(file_[-6:-4])-1\n",
        "\n",
        "            os.rename(''.join([root,'/',file_]), f'aclImdb/test/{dir_}/{file_}')\n",
        "\n",
        "!rm -r aclImdb/test/neg\n",
        "!rm -r aclImdb/test/pos"
      ],
      "metadata": {
        "id": "heJiLHekVD8_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls aclImdb/test"
      ],
      "metadata": {
        "id": "PO2P4FRFVEAj",
        "outputId": "5d1a3998-8f26-4c9c-c64e-bbc46ae7f2c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  1  2  3  4  5  6  7\t8  9  labeledBow.feat  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U68J6LUjVKsX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dummy_train_labels = np_utils.to_categorical(train_labels)"
      ],
      "metadata": {
        "id": "3jpoH7HF7bsA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KPrukP_qr3Lr",
        "outputId": "dcb99f14-a323-4ac6-c402-93091c352f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 10 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 10 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 10 classes.\n",
            "Number of batches in raw_train_ds: 1250\n",
            "Number of batches in raw_val_ds: 313\n",
            "Number of batches in raw_test_ds: 1563\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size)\n",
        "\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "  print(\"Label\", i, \"corresponds to\", label)"
      ],
      "metadata": {
        "id": "L_-tCO-gE0yL",
        "outputId": "8a4060ff-4ec7-4490-b5c1-397ecc4b885e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0 corresponds to 0\n",
            "Label 1 corresponds to 1\n",
            "Label 2 corresponds to 2\n",
            "Label 3 corresponds to 3\n",
            "Label 4 corresponds to 4\n",
            "Label 5 corresponds to 5\n",
            "Label 6 corresponds to 6\n",
            "Label 7 corresponds to 7\n",
            "Label 8 corresponds to 8\n",
            "Label 9 corresponds to 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zHKfpjEr3Lr"
      },
      "source": [
        "Let's preview a few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jqR1cFg5r3Lr",
        "outputId": "800d5667-4e7a-41a2-8325-8cd50a92edb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"I think that Elisabeth Rohm, though she may try hard, is not very good at all. I guess it was because of budget that she may have been the only one they could get for that price. I mainly watched it for the performance of Myron Natwick, whose work I know very well.<br /><br />He was the most believable and without doubt the most compelling to watch. When he wasn't on the screen, the thing went dead. This was filmed in Vancouver. He gave me the creeps, but be assured that in real life he is a kind, funny compassionate man. He even said playing that role gave him the creeps.<br /><br />I'll watch anything with him in it, but Elisabeth Rohm - never again. She was as exciting as lint on Law and Order. Maybe a very nice person, but no actress.\"\n",
            "1\n",
            "b'Boris Karloff is Matthias Morteval, a dying, lonely old nut who lives in Morhenge Mansion with some servants and tells his doctor friend, \"Don\\'t try to doctor me, doctor! I\\'m disgustingly healthy!\" He invites his nieces and nephews to his home and warns them they may have inherited a genetic disease that causes madness by \"shrinking the brain\" (?)<br /><br />***SPOILERS***<br /><br />Morteval/Karloff ends up dying, and murderous \"toys\" (designed by his dead brother) start killing off the relatives. A mini cannon fires real bullets into a guys face, a life-sized knight in armor attacks with an axe and a dancing Sheik stabs people with a knife. One guy getting strangled makes some hilarious faces. At the end, Julissa and her boyfriend find Karloff is still alive and hiding out in the dungeon where steel gates seal off the room. He plays the recurring organ theme music (sort of a death rattle used for the killings), the brother\\'s spirit starts talking (\"The whole house will go with me!\") and the mansion goes up in flames.<br /><br />This senseless mess is too dark, boring and the stupid dialogue never matches the lips.'\n",
            "0\n",
            "b'To confess having fantasies about Brad Pitt is a pretty tough admission for an heterosexual to make. But what can I tell you? Maybe is that famous extra something that everybody talks about and makes a star a star. It crosses that barrier. It pulls you into unknown sensual and emotional territory. Brando had it in spades, Montgomery Clift, Gary Cooper, James Dean of course and in more recent times, Tom Cruise, Jude Law, Johnny Depp, Ewan McGregor and Billy Crudup. Women fell in love with Garbo, Dietrich, Katharine and Audrey Hepburn, Grace Kelly, Marilyn Monroe, Julie Christie, Charlotte Rampling, Meryl Streep, Vanessa Redgrave, Julia Roberts and very very recently Natalie Portman. But Brad Pitt has, singlehandedly, redefined the concept. He is the only reason to go out, get in the car, find parking, buy a ticket, popcorn and get into a theatre to see \"Troy\" If you liked epics in the \"Jupiter\\'s Darling\" style you may enjoy this. But if you don\\'t, go all the same, we want to keep Brad Pitt in business.'\n",
            "1\n",
            "b'The 1st season was amazing, the whole idea of them adjusting to the island, while mysteries were being explored (And seen) was just phenomenal; filled with suspense, tons of cliffhangers, and an amazing plot. I mean, I love the whole idea of just seeing them get used to the island. And then first seeing the smoke monster in the first episode really caught my attention. From then on, I was hooked The second season was right on par with the 1st season, only a little better. I absolutely loved the idea of the hatches and the DHARMA Initiative. The whole plot and sequences of season 2 were mysterious, creepy, and exciting. I loved all the suspense surround others on the island, but the DHARMA story really made season 2 amazing.<br /><br />Season 3 wasn\\'t quite as good as 1 and 2 ... but nonetheless, great. I loved seeing the back-stories of the others, seeing their camp, and seeing the mysteries further explored. (\"Tricia Tanak Is Dead\" is one of my favorite episodes). This season, while not as good, was still breathtaking and fun, but most of all exciting! Now, the 4th season. I had hopes for this season, and the 1st couple of episodes we\\'re good, but then it REALLY started to get boring and monotonous. I mean, I REALLY despise the new \"rescuers\" such as Miles and Daniel. The plot got old after the first couple episodes ... and MOST OF ALL .... Season 4 was stripped away of something which made LOST a perfect series: The mystery, suspense, comedy mixed in (Charlie gone) and overall excitement. Also, some of my favorite characters have left.'\n",
            "9\n",
            "b'\\'\\'Ranma \\xc2\\xbd\" is my favorite anime by Rumiko Takahashi. The woman really knows how to entertain us with a good story, that is not only a comedy, but also an action anime. The main character of the story is Ranma Saotome, a teenager boy who is also an expert in martial arts. Ranma is engaged to Akane because of an arrangement of both fathers, who are great friends and trained together during many years. <br /><br />Akane is the younger and most violent sister of the Tendo\\'s: Kasumi is the oldest and is very sweet and Nabiki is the middle and loves to win money no matter what.<br /><br />Ranma and Akane fight all the time,specially because both have a very bad temper, and when they discover that Ranma becomes a girl when splashed with cold water as well as his father becomes a panda,many new characters and situations starts to happen. They also discover the reason of the transformation: while fighting, Ranma and his father fell in a cursed river. But not only them had this kind of fate...<br /><br />If you watched \\'\\'Ranma 1/2\\'\\' and liked, I would recommend you \\'\\'Inuyasha\\'\\' and \\'\\'Maison Ikkoku\", two other good creations from Rumiko\\'s hands.'\n",
            "7\n"
          ]
        }
      ],
      "source": [
        "for comments, labels in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(comments.numpy()[i])\n",
        "        print(labels.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCkHi9C7r3Ls"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "In particular, we remove `<br />` tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bVbJSo-sr3Ls",
        "outputId": "334d1110-b9d6-4678-f9e6-d370a032d805",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Having looked at our data above, we see that the raw text contains HTML break\n",
        "# tags of the form '<br />'. These tags will not be removed by the default\n",
        "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
        "# create a custom standardization function.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Model constants.\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "# Now that we have our custom standardization, we can instantiate our text\n",
        "# vectorization layer. We are using this layer to normalize, split, and map\n",
        "# strings to integers, so we set our 'output_mode' to 'int'.\n",
        "# Note that we're using the default split function,\n",
        "# and the custom standardization defined above.\n",
        "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
        "# model won't support ragged sequences.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63NUxMv_r3Ls"
      },
      "source": [
        "## Two options to vectorize the data\n",
        "\n",
        "There are 2 ways we can use our text vectorization layer:\n",
        "\n",
        "**Option 1: Make it part of the model**, so as to obtain a model that processes raw\n",
        " strings, like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_btvQZyyr3Ls"
      },
      "source": [
        "```python\n",
        "text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
        "x = vectorize_layer(text_input)\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
        "...\n",
        "```\n",
        "\n",
        "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n",
        " feed it into a model that expects integer sequences as inputs.\n",
        "\n",
        "An important difference between the two is that option 2 enables you to do\n",
        "**asynchronous CPU processing and buffering** of your data when training on GPU.\n",
        "So if you're training the model on GPU, you probably want to go with this option to get\n",
        " the best performance. This is what we will do below.\n",
        "\n",
        "If we were to export our model to production, we'd ship a model that accepts raw\n",
        "strings as input, like in the code snippet for option 1 above. This can be done after\n",
        " training. We do this in the last section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dA1UOemmr3Lt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMQZ7dLkr3Lt"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We choose a simple 1D convnet starting with an `Embedding` layer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "4mWNkJebAaav"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "90KgF_aar3Lt"
      },
      "outputs": [],
      "source": [
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "#x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "# x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "# x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.LSTM(128, return_sequences=True, return_state=False)(x)\n",
        "x = layers.LSTM(256, return_sequences=True, return_state=False)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "#x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Conv1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')(x)\n",
        "x = layers.AveragePooling1D(pool_size = 2)(x)\n",
        "x = layers.Bidirectional(LSTM(200, dropout = 0.5))(x)\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "TRswSwsh6qui"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "###x = layers.Embedding(129892, 32)(inputs)\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Bidirectional(LSTM(75, dropout = 0.1))(x)\n",
        "predictions = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# self.embed = T.nn.Embedding(129892, 32)\n",
        "# self.lstm = T.nn.LSTM(32, 75)\n",
        "# self.drop = T.nn.Dropout(0.10)\n",
        "# self.fc1 = T.nn.Linear(75, 10)  \n",
        "# self.fc2 = T.nn.Linear(10, 2)  # 0=neg, 1=pos\n",
        "\n",
        "# def forward(self, x):\n",
        "# # x = review/sentence. length = 50 (fixed w/ padding)\n",
        "# z = self.embed(x) \n",
        "# z = z.view(50, 1, 32)  # \"seq batch input\"\n",
        "# lstm_oupt, (h_n, c_n) = self.lstm(z)\n",
        "# z = lstm_oupt[-1]\n",
        "# z = self.drop(z)\n",
        "# z = T.tanh(self.fc1(z)) \n",
        "# z = self.fc2(z)  # CrossEntropyLoss will apply softmax\n",
        "# return z  "
      ],
      "metadata": {
        "id": "8Scntt66AX-Z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "predictions = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "pscDGuxJ8xSm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZE2mZPer3Lt"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xCF705t4r3Lt",
        "outputId": "82f8f97a-8228-47b1-b06f-305821757c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 124s 94ms/step - loss: 1.8418 - accuracy: 0.3067 - val_loss: 1.7912 - val_accuracy: 0.3298\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 64s 51ms/step - loss: 1.6300 - accuracy: 0.3852 - val_loss: 1.6618 - val_accuracy: 0.3634\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 63s 50ms/step - loss: 1.4508 - accuracy: 0.4391 - val_loss: 1.6665 - val_accuracy: 0.3644\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 1.3044 - accuracy: 0.4942 - val_loss: 1.6619 - val_accuracy: 0.3678\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 1.1938 - accuracy: 0.5315 - val_loss: 1.7194 - val_accuracy: 0.3802\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 64s 51ms/step - loss: 1.0724 - accuracy: 0.5755 - val_loss: 1.8287 - val_accuracy: 0.3774\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.9369 - accuracy: 0.6384 - val_loss: 2.0353 - val_accuracy: 0.3652\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 64s 51ms/step - loss: 0.8172 - accuracy: 0.6867 - val_loss: 2.1118 - val_accuracy: 0.3642\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.7265 - accuracy: 0.7285 - val_loss: 2.2940 - val_accuracy: 0.3720\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 70s 56ms/step - loss: 0.6226 - accuracy: 0.7683 - val_loss: 2.4960 - val_accuracy: 0.3418\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa4041a2160>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "epochs = 10\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRis13fMr3Lu"
      },
      "source": [
        "## Evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LzN-q4Jhr3Lu",
        "outputId": "a1fc73b6-b6fb-4db8-f3c4-378812245419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 21s 13ms/step - loss: 3.3232 - accuracy: 0.3380\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.3231983184814453, 0.33803999423980713]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_OjCFror3Lu"
      },
      "source": [
        "## Make an end-to-end model\n",
        "\n",
        "If you want to obtain a model capable of processing raw strings, you can simply\n",
        "create a new model (using the weights we just trained):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gZI5P-bLr3Lu",
        "outputId": "df5ec33d-0467-4a5b-f449-6e2a73ef27cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-174aea9dd713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Test it with `raw_test_ds`, which yields raw strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mend_to_end_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_test_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1758, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 2176, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5680, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 10) vs (None, 1)).\n"
          ]
        }
      ],
      "source": [
        "# A string input\n",
        "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "# Turn strings into vocab indices\n",
        "indices = vectorize_layer(inputs)\n",
        "# Turn vocab indices into predictions\n",
        "outputs = model(indices)\n",
        "\n",
        "# Our end to end model\n",
        "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_ = model.predict(test_ds)"
      ],
      "metadata": {
        "id": "gyJ_72rksubT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_"
      ],
      "metadata": {
        "id": "NI5ilMuqsu13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c3mVbRS2uz3C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_from_scratch",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}